{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import timeit\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path,'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data:  37646\n",
      "Size of test data:  18932\n"
     ]
    }
   ],
   "source": [
    "# change to your own path\n",
    "path = \"/home/neuralnet/NTU_60/\"\n",
    "#train data\n",
    "# train_data = load_data(path+'cross_subject_data/trans_train_data.pkl')\n",
    "# test_data = load_data(path+'cross_subject_data/trans_test_data.pkl')\n",
    "train_data = load_data(path+'cross_view_data/trans_train_data.pkl')\n",
    "test_data = load_data(path+'cross_view_data/trans_test_data.pkl')\n",
    "print(\"Size of training data: \", len(train_data))\n",
    "print(\"Size of test data: \", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_video(video):\n",
    "    max_75 = np.amax(video, axis=0)\n",
    "    min_75 = np.amin(video, axis=0)\n",
    "    max_x = np.max([max_75[i] for i in range(0,75,3)])\n",
    "    max_y = np.max([max_75[i] for i in range(1,75,3)])\n",
    "    max_z = np.max([max_75[i] for i in range(2,75,3)])\n",
    "    min_x = np.min([min_75[i] for i in range(0,75,3)])\n",
    "    min_y = np.min([min_75[i] for i in range(1,75,3)])\n",
    "    min_z = np.min([min_75[i] for i in range(2,75,3)])\n",
    "    norm = np.zeros_like(video)\n",
    "    for i in range(0,75,3):\n",
    "        norm[:,i] = 2*(video[:,i]-min_x)/(max_x-min_x)-1\n",
    "        norm[:,i+1] = 2*(video[:,i+1]-min_y)/(max_y-min_y)-1\n",
    "        norm[:,i+2] = 2*(video[:,i+2]-min_z)/(max_z-min_z)-1\n",
    "    return norm\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i]['input'] = normalize_video(np.array(train_data[i]['input']))\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i]['input'] = normalize_video(np.array(test_data[i]['input']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "dsamp_train = []\n",
    "for i in range(len(train_data)):\n",
    "    \n",
    "    val = np.asarray(train_data[i]['input'])\n",
    "    if val.shape[0] > 50:\n",
    "        new_val = np.zeros((50, 75))\n",
    "        diff = math.floor(val.shape[0]/50)\n",
    "        idx = 0\n",
    "        for i in range(0, val.shape[0], diff):\n",
    "            new_val[idx, :] = val[i, :]\n",
    "            idx += 1\n",
    "            if idx >= 50:\n",
    "                break\n",
    "        dsamp_train.append(new_val)\n",
    "    else:\n",
    "        dsamp_train.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsamp_test = []\n",
    "for i in range(len(test_data)):\n",
    "    val = np.asarray(test_data[i]['input'])\n",
    "    if val.shape[0] > 50:\n",
    "        new_val = np.zeros((50, 75))\n",
    "        diff = math.floor(val.shape[0]/50)\n",
    "        idx = 0\n",
    "        for i in range(0, val.shape[0], diff):\n",
    "            new_val[idx, :] = val[i, :]\n",
    "            idx += 1\n",
    "            if idx >= 50:\n",
    "                break\n",
    "        dsamp_test.append(new_val)\n",
    "    else:\n",
    "        dsamp_test.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell import RNNCell\n",
    "class LinearSpaceDecoderWrapper(RNNCell):\n",
    "  \"\"\"Operator adding a linear encoder to an RNN cell\"\"\"\n",
    "\n",
    "  def __init__(self, cell, output_size):\n",
    "    \"\"\"Create a cell with with a linear encoder in space.\n",
    "\n",
    "    Args:\n",
    "      cell: an RNNCell. The input is passed through a linear layer.\n",
    "\n",
    "    Raises:\n",
    "      TypeError: if cell is not an RNNCell.\n",
    "    \"\"\"\n",
    "    if not isinstance(cell, RNNCell):\n",
    "      raise TypeError(\"The parameter cell is not a RNNCell.\")\n",
    "\n",
    "    self._cell = cell\n",
    "\n",
    "    print( 'output_size = {0}'.format(output_size) )\n",
    "    print( ' state_size = {0}'.format(self._cell.state_size) )\n",
    "\n",
    "    # Tuple if multi-rnn\n",
    "    if isinstance(self._cell.state_size,tuple):\n",
    "\n",
    "      # Fine if GRU...\n",
    "      insize = self._cell.state_size[-1]\n",
    "\n",
    "      # LSTMStateTuple if LSTM\n",
    "      if isinstance( insize, LSTMStateTuple ):\n",
    "        insize = insize.h\n",
    "\n",
    "    else:\n",
    "      # Fine if not multi-rnn\n",
    "      insize = self._cell.state_size\n",
    "\n",
    "    self.w_out = tf.get_variable(\"proj_w_out\",\n",
    "        [insize, output_size],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.random_uniform_initializer(minval=-0.04, maxval=0.04))\n",
    "    self.b_out = tf.get_variable(\"proj_b_out\", [output_size],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.random_uniform_initializer(minval=-0.04, maxval=0.04))\n",
    "\n",
    "    self.linear_output_size = output_size\n",
    "\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._cell.state_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self.linear_output_size\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Use a linear layer and pass the output to the cell.\"\"\"\n",
    "\n",
    "    # Run the rnn as usual\n",
    "    output, new_state = self._cell(inputs, state, scope)\n",
    "\n",
    "    # Apply the multiplication to everything\n",
    "    output = tf.matmul(output, self.w_out) + self.b_out\n",
    "\n",
    "    return output, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualWrapper(RNNCell):\n",
    "  \"\"\"Operator adding residual connections to a given cell.\"\"\"\n",
    "\n",
    "  def __init__(self, cell):\n",
    "    \"\"\"Create a cell with added residual connection.\n",
    "\n",
    "    Args:\n",
    "      cell: an RNNCell. The input is added to the output.\n",
    "\n",
    "    Raises:\n",
    "      TypeError: if cell is not an RNNCell.\n",
    "    \"\"\"\n",
    "    if not isinstance(cell, RNNCell):\n",
    "      raise TypeError(\"The parameter cell is not a RNNCell.\")\n",
    "\n",
    "    self._cell = cell\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._cell.state_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._cell.output_size\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Run the cell and add a residual connection.\"\"\"\n",
    "\n",
    "    # Run the rnn as usual\n",
    "    output, new_state = self._cell(inputs, state, scope)\n",
    "\n",
    "    # Add the residual connection\n",
    "    output = tf.add(output, inputs)\n",
    "\n",
    "    return output, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn import _transpose_batch_time\n",
    "class Seq2SeqModelFS(object):\n",
    "    def __init__(self, max_seq_len, input_size, rnn_size, batch_size, lr, train_keep_prob,decay_rate=0.95,dtype=tf.float32):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.rnn_size = rnn_size\n",
    "        self.batch_size = tf.placeholder_with_default(batch_size,shape=())\n",
    "        self.input_size = input_size\n",
    "        self.lr = tf.Variable( float(lr), trainable=False, dtype=dtype )\n",
    "        self.learning_rate_decay_op = self.lr.assign( self.lr * decay_rate)\n",
    "        self.keep_prob = tf.placeholder_with_default(1.0,shape=())\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        print('rnn_size = {0}'.format(rnn_size))\n",
    "        \n",
    "        with tf.variable_scope(\"inputs\"):\n",
    "            self.enc_xyz = tf.placeholder(dtype, shape=[None, self.max_seq_len, input_size], name='enc_xyz')\n",
    "            self.dec_xyz = tf.placeholder(dtype, shape=[None, self.max_seq_len, input_size], name='dec_xyz')\n",
    "            self.seq_len = tf.placeholder(tf.int32,[None])\n",
    "            mask = tf.sign(tf.reduce_max(tf.abs(self.enc_xyz[:,1:,:]), 2))\n",
    "\n",
    "        with tf.variable_scope(\"prediction\"):\n",
    "            with tf.variable_scope(\"encoder\"):\n",
    "                with tf.variable_scope(\"encoder_xyz\",reuse=tf.AUTO_REUSE):\n",
    "                    cell_fw_xyz = [tf.nn.rnn_cell.GRUCell(self.rnn_size//2) for i in range(3)]\n",
    "                    cell_bw_xyz = [tf.nn.rnn_cell.GRUCell(self.rnn_size//2) for i in range(3)]\n",
    "                    tuple_xyz = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cell_fw_xyz,cell_bw_xyz,self.enc_xyz,dtype=tf.float32,sequence_length=self.seq_len)\n",
    "                    bi_xyz_h = tf.concat((tuple_xyz[1][-1],tuple_xyz[2][-1]),-1)\n",
    "                    self.bi_xyz_h = bi_xyz_h\n",
    "            self.knn_state = self.bi_xyz_h\n",
    "            with tf.variable_scope(\"decoder\"):\n",
    "                with tf.variable_scope(\"decoder_xyz\",reuse=tf.AUTO_REUSE):\n",
    "                    cell_xyz__ = tf.nn.rnn_cell.GRUCell(self.rnn_size)\n",
    "                    cell_xyz_ = LinearSpaceDecoderWrapper(cell_xyz__,self.input_size)\n",
    "                    cell_xyz = ResidualWrapper(cell_xyz_)\n",
    "                    def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "                        \"\"\"\n",
    "                        Loop function that allows to control input to the rnn cell and manipulate cell outputs.\n",
    "                        :param time: current time step\n",
    "                        :param cell_output: output from previous time step or None if time == 0\n",
    "                        :param cell_state: cell state from previous time step\n",
    "                        :param loop_state: custom loop state to share information between different iterations of this loop fn\n",
    "                        :return: tuple consisting of\n",
    "                          elements_finished: tensor of size [bach_size] which is True for sequences that have reached their end,\n",
    "                            needed because of variable sequence size\n",
    "                          next_input: input to next time step\n",
    "                          next_cell_state: cell state forwarded to next time step\n",
    "                          emit_output: The first return argument of raw_rnn. This is not necessarily the output of the RNN cell,\n",
    "                            but could e.g. be the output of a dense layer attached to the rnn layer.\n",
    "                          next_loop_state: loop state forwarded to the next time step\n",
    "                        \"\"\"\n",
    "                        if cell_output is None:\n",
    "                            # time == 0, used for initialization before first call to cell\n",
    "                            next_cell_state = self.bi_xyz_h\n",
    "                            # the emit_output in this case tells TF how future emits look\n",
    "                            emit_output = tf.zeros([self.input_size])\n",
    "                        else:\n",
    "                            # t > 0, called right after call to cell, i.e. cell_output is the output from time t-1.\n",
    "                            # here you can do whatever ou want with cell_output before assigning it to emit_output.\n",
    "                            # In this case, we don't do anything\n",
    "                            next_cell_state = self.bi_xyz_h#NOTE:IF NO-FS, use cell_state#\n",
    "                            emit_output = cell_output  \n",
    "\n",
    "                        # check which elements are finished\n",
    "                        elements_finished = (time >= self.seq_len-1)\n",
    "                        finished = tf.reduce_all(elements_finished)\n",
    "\n",
    "                        # assemble cell input for upcoming time step\n",
    "                        current_output = emit_output if cell_output is not None else None\n",
    "                        #input_original = inputs_ta.read(time)  # tensor of shape (None, input_dim)\n",
    "                        input_original = self.enc_xyz[:,0,:]\n",
    "                        if current_output is None:\n",
    "                            # this is the initial step, i.e. there is no output from a previous time step, what we feed here\n",
    "                            # can highly depend on the data. In this case we just assign the actual input in the first time step.\n",
    "                            next_in = input_original\n",
    "                        else:\n",
    "                            # time > 0, so just use previous output as next input\n",
    "                            # here you could do fancier things, whatever you want to do before passing the data into the rnn cell\n",
    "                            # if here you were to pass input_original than you would get the normal behaviour of dynamic_rnn\n",
    "                            next_in = current_output\n",
    "\n",
    "                        next_input = tf.cond(finished,\n",
    "                                             lambda: tf.zeros([self.batch_size, self.input_size], dtype=tf.float32),  # copy through zeros\n",
    "                                             lambda: next_in)  # if not finished, feed the previous output as next input\n",
    "\n",
    "                        # set shape manually, otherwise it is not defined for the last dimensions\n",
    "                        next_input.set_shape([None, self.input_size])\n",
    "\n",
    "                        # loop state not used in this example\n",
    "                        next_loop_state = None\n",
    "                        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "                    outputs_ta, def_final_state_xyz, _ = tf.nn.raw_rnn(cell_xyz, loop_fn)\n",
    "                    self.dec_outputs_xyz = _transpose_batch_time(outputs_ta.stack())\n",
    "            def loss_with_mask(pred,gt,mask):\n",
    "                loss = tf.reduce_sum(tf.abs(pred-gt),2)*mask\n",
    "                loss = tf.reduce_sum(loss,1)\n",
    "                loss /= tf.reduce_sum(mask,1)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "                return loss\n",
    "        with tf.variable_scope(\"pred_xyz\",reuse=tf.AUTO_REUSE):\n",
    "            pred_xyz2xyz = self.dec_outputs_xyz\n",
    "            self.loss = loss_with_mask(pred_xyz2xyz,self.enc_xyz[:,1:,:],mask)\n",
    "        \n",
    "        opt = tf.train.AdamOptimizer(self.lr)\n",
    "        gradients, self.pred_vars = zip(*opt.compute_gradients(self.loss))\n",
    "        clipped_gradients, norm = tf.clip_by_global_norm(gradients, 25)\n",
    "        self.updates = opt.apply_gradients(zip(clipped_gradients,self.pred_vars),global_step = self.global_step)\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "    \n",
    "    def step(self, session, encoder_inputs_xyz,decoder_inputs_xyz,seq_len,forward_only):\n",
    "        if not forward_only:\n",
    "            input_feed = {self.enc_xyz: encoder_inputs_xyz,\n",
    "                          self.dec_xyz: decoder_inputs_xyz,\n",
    "                          self.seq_len: seq_len}\n",
    "            output_feed = [self.updates,self.loss]\n",
    "            outputs = session.run(output_feed, input_feed)\n",
    "            return outputs[0], outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session():\n",
    "    \"\"\"Create a session that dynamically allocates memory.\"\"\"\n",
    "    # See: https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
    "    config = tf.ConfigProto(log_device_placement=True,allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_classify(feature_xyz,labels,seq_len,batch_size):\n",
    "    for start in range(0,len(feature_xyz),batch_size):\n",
    "        end = min(start+batch_size,len(feature_xyz))\n",
    "        yield feature_xyz[start:end],labels[start:end],seq_len[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len=50\n",
    "rnn_size=2048\n",
    "input_size = 75\n",
    "batch_size=64\n",
    "lr = .0001\n",
    "train_keep_prob = 1.0\n",
    "iterations = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_size = 2048\n",
      "output_size = 75\n",
      " state_size = 2048\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# FW\n",
    "model = Seq2SeqModelFS(max_seq_len, input_size,rnn_size, batch_size, lr,train_keep_prob)\n",
    "sess = get_session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the acc before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea = []\n",
    "lab = []\n",
    "seq_len_new = []\n",
    "for idx, data in enumerate(train_data):\n",
    "    label = data[\"label\"]\n",
    "    val = np.asarray(data[\"input\"])\n",
    "    raw_len = val.shape[0]\n",
    "    if raw_len > 50:\n",
    "        seq_len_new.append(50)\n",
    "        fea.append(dsamp_train[idx])\n",
    "    else:\n",
    "        seq_len_new.append(raw_len)\n",
    "        pad_data = np.zeros((50, 75))\n",
    "        pad_data[:raw_len, :] = dsamp_train[idx]\n",
    "        fea.append(pad_data)\n",
    "    one_hot_label = np.zeros((60,))\n",
    "    one_hot_label[label] = 1.\n",
    "    lab.append(one_hot_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fea = []\n",
    "test_lab = []\n",
    "test_seq_len_new = []\n",
    "for idx, data in enumerate(test_data):\n",
    "    label = data[\"label\"]\n",
    "    val = np.asarray(data[\"input\"])\n",
    "    raw_len = val.shape[0]\n",
    "    if raw_len > 50:\n",
    "        test_seq_len_new.append(50)\n",
    "        test_fea.append(dsamp_test[idx])\n",
    "    else:\n",
    "        test_seq_len_new.append(raw_len)\n",
    "        pad_data = np.zeros((50, 75))\n",
    "        pad_data[:raw_len, :] = dsamp_test[idx]\n",
    "        test_fea.append(pad_data)\n",
    "    one_hot_label = np.zeros((60,))\n",
    "    one_hot_label[label] = 1.\n",
    "    test_lab.append(one_hot_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(model,session,encoder_inputs,batch_size,seq_len):\n",
    "    input_feed = {model.enc_xyz: encoder_inputs, model.seq_len: seq_len, model.batch_size:batch_size}\n",
    "    output_feed = [model.knn_state]\n",
    "    outputs = session.run(output_feed, input_feed)\n",
    "    return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_feature = []\n",
    "for encoder_inputs, labels, seq_len_enc in mini_batch_classify(fea, lab, seq_len_new, batch_size=64):\n",
    "    start_time = timeit.default_timer()\n",
    "    result = get_feature(model,sess,encoder_inputs,len(encoder_inputs),seq_len_enc)    \n",
    "    knn_feature.append(result)\n",
    "\n",
    "test_knn_feature = []\n",
    "for encoder_inputs, labels, seq_len_enc in mini_batch_classify(test_fea, test_lab, test_seq_len_new, batch_size=64):\n",
    "    result = get_feature(model,sess,encoder_inputs,len(encoder_inputs),seq_len_enc)\n",
    "    test_knn_feature.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_feature = np.vstack(knn_feature)\n",
    "test_knn_feature = np.vstack(test_knn_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='cosine',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=1,metric='cosine')\n",
    "neigh.fit(knn_feature,np.argmax(lab,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5667124445383478"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.score(test_knn_feature,np.argmax(test_lab,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(data, seq_len, input_size, batch_size):\n",
    "    encoder_inputs = np.zeros((batch_size, seq_len, input_size),dtype=float)\n",
    "    seq_len_enc = np.zeros((batch_size,), dtype=float)\n",
    "    decoder_inputs = np.zeros((batch_size, seq_len, input_size),dtype=float)\n",
    "    data_len = len(data)\n",
    "    for i in range(batch_size):\n",
    "        index = np.random.choice(data_len)\n",
    "        data_sel = data[index]\n",
    "        encoder_inputs[i, :data_sel.shape[0], :] = np.copy(data_sel)\n",
    "        seq_len_enc[i] = data_sel.shape[0]\n",
    "    return encoder_inputs, decoder_inputs, seq_len_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100:train loss:13.5850\n",
      "iteration 100: using 97.12 sec\n",
      "step 200:train loss:10.0412\n",
      "iteration 200: using 94.71 sec\n",
      "knn test score at 200th iterations:  0.6197971688147053\n",
      "Current KNN Max Score is 0.6197971688147053\n",
      "step 300:train loss:8.3745\n",
      "iteration 300: using 197.96 sec\n",
      "step 400:train loss:7.7679\n",
      "iteration 400: using 95.95 sec\n",
      "knn test score at 400th iterations:  0.6394992605113036\n",
      "Current KNN Max Score is 0.6394992605113036\n",
      "step 500:train loss:7.2031\n",
      "iteration 500: using 196.44 sec\n",
      "step 600:train loss:6.8727\n",
      "iteration 600: using 95.73 sec\n",
      "knn test score at 600th iterations:  0.6585146841326854\n",
      "Current KNN Max Score is 0.6585146841326854\n",
      "step 700:train loss:6.6160\n",
      "iteration 700: using 197.47 sec\n",
      "step 800:train loss:6.9852\n",
      "iteration 800: using 95.55 sec\n",
      "knn test score at 800th iterations:  0.6701352207901965\n",
      "Current KNN Max Score is 0.6701352207901965\n",
      "step 900:train loss:6.8098\n",
      "iteration 900: using 198.44 sec\n",
      "step 1000:train loss:6.2769\n",
      "iteration 1000: using 95.93 sec\n",
      "knn test score at 1000th iterations:  0.6821255017959011\n",
      "Current KNN Max Score is 0.6821255017959011\n",
      "step 1100:train loss:6.5095\n",
      "iteration 1100: using 199.19 sec\n",
      "step 1200:train loss:6.0500\n",
      "iteration 1200: using 96.02 sec\n",
      "knn test score at 1200th iterations:  0.6862455102472005\n",
      "Current KNN Max Score is 0.6862455102472005\n",
      "step 1300:train loss:6.2390\n",
      "iteration 1300: using 197.44 sec\n",
      "step 1400:train loss:6.2325\n",
      "iteration 1400: using 95.65 sec\n",
      "knn test score at 1400th iterations:  0.6926368054088317\n",
      "Current KNN Max Score is 0.6926368054088317\n",
      "step 1500:train loss:6.2412\n",
      "iteration 1500: using 198.84 sec\n",
      "step 1600:train loss:5.8923\n",
      "iteration 1600: using 96.62 sec\n",
      "knn test score at 1600th iterations:  0.6952778364673569\n",
      "Current KNN Max Score is 0.6952778364673569\n",
      "step 1700:train loss:5.9255\n",
      "iteration 1700: using 199.49 sec\n",
      "step 1800:train loss:6.0515\n",
      "iteration 1800: using 97.08 sec\n",
      "knn test score at 1800th iterations:  0.6971265582083245\n",
      "Current KNN Max Score is 0.6971265582083245\n",
      "step 1900:train loss:5.8597\n",
      "iteration 1900: using 197.87 sec\n",
      "step 2000:train loss:5.7185\n",
      "iteration 2000: using 96.51 sec\n",
      "knn test score at 2000th iterations:  0.701457849144306\n",
      "Current KNN Max Score is 0.701457849144306\n",
      "step 2100:train loss:6.0631\n",
      "iteration 2100: using 198.68 sec\n",
      "step 2200:train loss:5.7098\n",
      "iteration 2200: using 96.89 sec\n",
      "knn test score at 2200th iterations:  0.7033593915064441\n",
      "Current KNN Max Score is 0.7033593915064441\n",
      "step 2300:train loss:5.6245\n",
      "iteration 2300: using 197.41 sec\n",
      "step 2400:train loss:6.0990\n",
      "iteration 2400: using 95.82 sec\n",
      "knn test score at 2400th iterations:  0.7070568349883795\n",
      "Current KNN Max Score is 0.7070568349883795\n",
      "step 2500:train loss:5.6805\n",
      "iteration 2500: using 199.57 sec\n",
      "step 2600:train loss:5.7465\n",
      "iteration 2600: using 96.01 sec\n",
      "knn test score at 2600th iterations:  0.7116522290302134\n",
      "Current KNN Max Score is 0.7116522290302134\n",
      "step 2700:train loss:5.6123\n",
      "iteration 2700: using 199.66 sec\n",
      "step 2800:train loss:5.5122\n",
      "iteration 2800: using 95.79 sec\n",
      "knn test score at 2800th iterations:  0.7159306993450243\n",
      "Current KNN Max Score is 0.7159306993450243\n",
      "step 2900:train loss:5.6512\n",
      "iteration 2900: using 200.96 sec\n",
      "step 3000:train loss:5.2816\n",
      "iteration 3000: using 96.72 sec\n",
      "knn test score at 3000th iterations:  0.7154024931333193\n",
      "step 3100:train loss:5.5905\n",
      "iteration 3100: using 197.28 sec\n",
      "step 3200:train loss:5.6260\n",
      "iteration 3200: using 96.00 sec\n",
      "knn test score at 3200th iterations:  0.7165117261778998\n",
      "Current KNN Max Score is 0.7165117261778998\n",
      "step 3300:train loss:5.5034\n",
      "iteration 3300: using 200.49 sec\n",
      "step 3400:train loss:5.5238\n",
      "iteration 3400: using 97.21 sec\n",
      "knn test score at 3400th iterations:  0.7190999366152546\n",
      "Current KNN Max Score is 0.7190999366152546\n",
      "step 3500:train loss:5.8387\n",
      "iteration 3500: using 201.17 sec\n",
      "step 3600:train loss:5.4542\n",
      "iteration 3600: using 96.99 sec\n",
      "knn test score at 3600th iterations:  0.7215296851890978\n",
      "Current KNN Max Score is 0.7215296851890978\n",
      "step 3700:train loss:5.3130\n",
      "iteration 3700: using 200.17 sec\n",
      "step 3800:train loss:5.1122\n",
      "iteration 3800: using 95.20 sec\n",
      "knn test score at 3800th iterations:  0.7223219945066554\n",
      "Current KNN Max Score is 0.7223219945066554\n",
      "step 3900:train loss:5.4452\n",
      "iteration 3900: using 200.04 sec\n",
      "step 4000:train loss:5.2171\n",
      "iteration 4000: using 96.33 sec\n",
      "knn test score at 4000th iterations:  0.7248045637016691\n",
      "Current KNN Max Score is 0.7248045637016691\n",
      "step 4100:train loss:5.0841\n",
      "iteration 4100: using 199.56 sec\n",
      "step 4200:train loss:5.1980\n",
      "iteration 4200: using 96.38 sec\n",
      "knn test score at 4200th iterations:  0.7220050707796324\n",
      "step 4300:train loss:5.2316\n",
      "iteration 4300: using 199.11 sec\n",
      "step 4400:train loss:5.2058\n",
      "iteration 4400: using 95.97 sec\n",
      "knn test score at 4400th iterations:  0.7239066131417705\n",
      "step 4500:train loss:4.9965\n",
      "iteration 4500: using 199.23 sec\n",
      "step 4600:train loss:5.2990\n",
      "iteration 4600: using 96.39 sec\n",
      "knn test score at 4600th iterations:  0.7243291781111346\n",
      "step 4700:train loss:5.4307\n",
      "iteration 4700: using 200.07 sec\n",
      "step 4800:train loss:5.3749\n",
      "iteration 4800: using 96.03 sec\n",
      "knn test score at 4800th iterations:  0.7249630255651807\n",
      "Current KNN Max Score is 0.7249630255651807\n",
      "step 4900:train loss:5.0944\n",
      "iteration 4900: using 198.86 sec\n",
      "step 5000:train loss:5.2658\n",
      "iteration 5000: using 96.34 sec\n",
      "knn test score at 5000th iterations:  0.7259137967462498\n",
      "Current KNN Max Score is 0.7259137967462498\n",
      "step 5100:train loss:5.0030\n",
      "iteration 5100: using 199.70 sec\n",
      "step 5200:train loss:5.0336\n",
      "iteration 5200: using 95.64 sec\n",
      "knn test score at 5200th iterations:  0.7257025142615677\n",
      "step 5300:train loss:5.0372\n",
      "iteration 5300: using 200.69 sec\n",
      "step 5400:train loss:5.0620\n",
      "iteration 5400: using 97.71 sec\n",
      "knn test score at 5400th iterations:  0.7264948235791253\n",
      "Current KNN Max Score is 0.7264948235791253\n",
      "step 5500:train loss:5.0592\n",
      "iteration 5500: using 199.22 sec\n",
      "step 5600:train loss:4.9063\n",
      "iteration 5600: using 94.74 sec\n",
      "knn test score at 5600th iterations:  0.7277096978660469\n",
      "Current KNN Max Score is 0.7277096978660469\n",
      "step 5700:train loss:5.0874\n",
      "iteration 5700: using 200.82 sec\n",
      "step 5800:train loss:5.1053\n",
      "iteration 5800: using 96.60 sec\n",
      "knn test score at 5800th iterations:  0.7294527783646736\n",
      "Current KNN Max Score is 0.7294527783646736\n",
      "step 5900:train loss:4.7915\n",
      "iteration 5900: using 202.03 sec\n",
      "step 6000:train loss:4.9566\n",
      "iteration 6000: using 96.24 sec\n",
      "knn test score at 6000th iterations:  0.729294316501162\n",
      "step 6100:train loss:5.0821\n",
      "iteration 6100: using 200.75 sec\n",
      "step 6200:train loss:4.9799\n",
      "iteration 6200: using 97.26 sec\n",
      "knn test score at 6200th iterations:  0.7308789351362772\n",
      "Current KNN Max Score is 0.7308789351362772\n",
      "step 6300:train loss:5.1579\n",
      "iteration 6300: using 201.42 sec\n",
      "step 6400:train loss:4.6824\n",
      "iteration 6400: using 96.53 sec\n",
      "knn test score at 6400th iterations:  0.7321466300443693\n",
      "Current KNN Max Score is 0.7321466300443693\n",
      "step 6500:train loss:4.9770\n",
      "iteration 6500: using 200.70 sec\n",
      "step 6600:train loss:4.9621\n",
      "iteration 6600: using 96.71 sec\n",
      "knn test score at 6600th iterations:  0.733889710542996\n",
      "Current KNN Max Score is 0.733889710542996\n",
      "step 6700:train loss:4.6640\n",
      "iteration 6700: using 201.82 sec\n",
      "step 6800:train loss:4.7670\n",
      "iteration 6800: using 96.65 sec\n",
      "knn test score at 6800th iterations:  0.7358440735263047\n",
      "Current KNN Max Score is 0.7358440735263047\n",
      "step 6900:train loss:4.8593\n",
      "iteration 6900: using 201.23 sec\n",
      "step 7000:train loss:4.8971\n",
      "iteration 7000: using 96.99 sec\n",
      "knn test score at 7000th iterations:  0.7363194591168392\n",
      "Current KNN Max Score is 0.7363194591168392\n",
      "step 7100:train loss:4.6561\n",
      "iteration 7100: using 201.39 sec\n",
      "step 7200:train loss:4.6465\n",
      "iteration 7200: using 95.77 sec\n",
      "knn test score at 7200th iterations:  0.7379568983731248\n",
      "Current KNN Max Score is 0.7379568983731248\n",
      "step 7300:train loss:4.9404\n",
      "iteration 7300: using 199.36 sec\n",
      "step 7400:train loss:4.6522\n",
      "iteration 7400: using 96.20 sec\n",
      "knn test score at 7400th iterations:  0.7382210014789774\n",
      "Current KNN Max Score is 0.7382210014789774\n",
      "step 7500:train loss:4.5329\n",
      "iteration 7500: using 200.22 sec\n",
      "step 7600:train loss:4.7335\n",
      "iteration 7600: using 96.66 sec\n",
      "knn test score at 7600th iterations:  0.7394886963870695\n",
      "Current KNN Max Score is 0.7394886963870695\n",
      "step 7700:train loss:4.7520\n",
      "iteration 7700: using 201.13 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7800:train loss:4.7999\n",
      "iteration 7800: using 97.25 sec\n",
      "knn test score at 7800th iterations:  0.739435875765899\n",
      "step 7900:train loss:4.5526\n",
      "iteration 7900: using 201.90 sec\n",
      "step 8000:train loss:4.6053\n",
      "iteration 8000: using 97.17 sec\n",
      "knn test score at 8000th iterations:  0.7412845975068667\n",
      "Current KNN Max Score is 0.7412845975068667\n",
      "step 8100:train loss:4.8429\n",
      "iteration 8100: using 201.72 sec\n",
      "step 8200:train loss:4.7566\n",
      "iteration 8200: using 97.09 sec\n",
      "knn test score at 8200th iterations:  0.7410733150221847\n",
      "step 8300:train loss:4.7267\n",
      "iteration 8300: using 203.78 sec\n",
      "step 8400:train loss:4.5659\n",
      "iteration 8400: using 96.47 sec\n",
      "knn test score at 8400th iterations:  0.7427107542784703\n",
      "Current KNN Max Score is 0.7427107542784703\n",
      "step 8500:train loss:4.5794\n",
      "iteration 8500: using 201.59 sec\n",
      "step 8600:train loss:4.6262\n",
      "iteration 8600: using 96.30 sec\n",
      "knn test score at 8600th iterations:  0.7421825480667653\n",
      "step 8700:train loss:4.7235\n",
      "iteration 8700: using 201.15 sec\n",
      "step 8800:train loss:4.7578\n",
      "iteration 8800: using 96.13 sec\n",
      "knn test score at 8800th iterations:  0.7419712655820833\n",
      "step 8900:train loss:4.5729\n",
      "iteration 8900: using 201.75 sec\n",
      "step 9000:train loss:4.3440\n",
      "iteration 9000: using 96.16 sec\n",
      "knn test score at 9000th iterations:  0.7435558842171984\n",
      "Current KNN Max Score is 0.7435558842171984\n",
      "step 9100:train loss:4.6924\n",
      "iteration 9100: using 201.21 sec\n",
      "step 9200:train loss:4.6195\n",
      "iteration 9200: using 96.65 sec\n",
      "knn test score at 9200th iterations:  0.7413374181280372\n",
      "step 9300:train loss:4.5099\n",
      "iteration 9300: using 200.31 sec\n",
      "step 9400:train loss:4.5237\n",
      "iteration 9400: using 96.30 sec\n",
      "knn test score at 9400th iterations:  0.7437671667018804\n",
      "Current KNN Max Score is 0.7437671667018804\n",
      "step 9500:train loss:4.5934\n",
      "iteration 9500: using 198.45 sec\n",
      "step 9600:train loss:4.5760\n",
      "iteration 9600: using 96.82 sec\n",
      "knn test score at 9600th iterations:  0.7444538347770969\n",
      "Current KNN Max Score is 0.7444538347770969\n",
      "step 9700:train loss:4.5290\n",
      "iteration 9700: using 199.49 sec\n",
      "step 9800:train loss:4.5239\n",
      "iteration 9800: using 96.33 sec\n",
      "knn test score at 9800th iterations:  0.7423938305514473\n",
      "step 9900:train loss:4.4067\n",
      "iteration 9900: using 201.89 sec\n",
      "step 10000:train loss:4.6773\n",
      "iteration 10000: using 96.66 sec\n",
      "knn test score at 10000th iterations:  0.7437143460807099\n",
      "step 10100:train loss:4.6078\n",
      "iteration 10100: using 199.98 sec\n",
      "step 10200:train loss:4.4345\n",
      "iteration 10200: using 96.28 sec\n",
      "knn test score at 10200th iterations:  0.745404605958166\n",
      "Current KNN Max Score is 0.745404605958166\n",
      "step 10300:train loss:4.5020\n",
      "iteration 10300: using 199.37 sec\n",
      "step 10400:train loss:4.5152\n",
      "iteration 10400: using 95.45 sec\n",
      "knn test score at 10400th iterations:  0.7446122966406085\n",
      "step 10500:train loss:4.4838\n",
      "iteration 10500: using 198.83 sec\n",
      "step 10600:train loss:4.5060\n",
      "iteration 10600: using 96.41 sec\n",
      "knn test score at 10600th iterations:  0.7437143460807099\n",
      "step 10700:train loss:4.3345\n",
      "iteration 10700: using 199.71 sec\n",
      "step 10800:train loss:4.3868\n",
      "iteration 10800: using 96.44 sec\n",
      "knn test score at 10800th iterations:  0.7451405028523136\n",
      "step 10900:train loss:4.3405\n",
      "iteration 10900: using 198.89 sec\n",
      "step 11000:train loss:4.5631\n",
      "iteration 11000: using 95.86 sec\n",
      "knn test score at 11000th iterations:  0.744242552292415\n",
      "step 11100:train loss:4.4879\n",
      "iteration 11100: using 199.49 sec\n",
      "step 11200:train loss:4.3593\n",
      "iteration 11200: using 96.96 sec\n",
      "knn test score at 11200th iterations:  0.744876399746461\n",
      "step 11300:train loss:4.4011\n",
      "iteration 11300: using 198.59 sec\n",
      "step 11400:train loss:4.2292\n",
      "iteration 11400: using 96.02 sec\n",
      "knn test score at 11400th iterations:  0.7458799915487007\n",
      "Current KNN Max Score is 0.7458799915487007\n",
      "step 11500:train loss:4.3519\n",
      "iteration 11500: using 200.30 sec\n",
      "step 11600:train loss:4.2853\n",
      "iteration 11600: using 96.61 sec\n",
      "knn test score at 11600th iterations:  0.7455630678216776\n",
      "step 11700:train loss:4.3687\n",
      "iteration 11700: using 199.21 sec\n",
      "step 11800:train loss:4.5742\n",
      "iteration 11800: using 96.15 sec\n",
      "knn test score at 11800th iterations:  0.7478871751531798\n",
      "Current KNN Max Score is 0.7478871751531798\n",
      "step 11900:train loss:4.3388\n",
      "iteration 11900: using 200.18 sec\n",
      "step 12000:train loss:4.3945\n",
      "iteration 12000: using 96.99 sec\n",
      "knn test score at 12000th iterations:  0.7474646101838157\n",
      "step 12100:train loss:4.2056\n",
      "iteration 12100: using 199.98 sec\n",
      "step 12200:train loss:4.4022\n",
      "iteration 12200: using 96.88 sec\n",
      "knn test score at 12200th iterations:  0.7472005070779633\n",
      "step 12300:train loss:4.3993\n",
      "iteration 12300: using 199.32 sec\n",
      "step 12400:train loss:4.1637\n",
      "iteration 12400: using 97.06 sec\n",
      "knn test score at 12400th iterations:  0.7464081977604057\n",
      "step 12500:train loss:4.2889\n",
      "iteration 12500: using 201.12 sec\n",
      "step 12600:train loss:4.3682\n",
      "iteration 12600: using 96.07 sec\n",
      "knn test score at 12600th iterations:  0.7482040988802028\n",
      "Current KNN Max Score is 0.7482040988802028\n",
      "step 12700:train loss:4.4066\n",
      "iteration 12700: using 200.00 sec\n",
      "step 12800:train loss:4.1415\n",
      "iteration 12800: using 95.69 sec\n",
      "knn test score at 12800th iterations:  0.7467779421085992\n",
      "step 12900:train loss:4.2380\n",
      "iteration 12900: using 200.93 sec\n",
      "step 13000:train loss:4.2148\n",
      "iteration 13000: using 96.86 sec\n",
      "knn test score at 13000th iterations:  0.7473061483203043\n",
      "step 13100:train loss:4.3354\n",
      "iteration 13100: using 198.87 sec\n",
      "step 13200:train loss:4.2860\n",
      "iteration 13200: using 95.46 sec\n",
      "knn test score at 13200th iterations:  0.7473589689414748\n",
      "step 13300:train loss:4.3394\n",
      "iteration 13300: using 198.92 sec\n",
      "step 13400:train loss:4.0876\n",
      "iteration 13400: using 96.51 sec\n",
      "knn test score at 13400th iterations:  0.7478343545320093\n",
      "step 13500:train loss:4.0952\n",
      "iteration 13500: using 198.20 sec\n",
      "step 13600:train loss:4.1958\n",
      "iteration 13600: using 96.16 sec\n",
      "knn test score at 13600th iterations:  0.7473061483203043\n",
      "step 13700:train loss:4.2419\n",
      "iteration 13700: using 200.31 sec\n",
      "step 13800:train loss:4.0767\n",
      "iteration 13800: using 96.06 sec\n",
      "knn test score at 13800th iterations:  0.7504753855905345\n",
      "Current KNN Max Score is 0.7504753855905345\n",
      "step 13900:train loss:4.0577\n",
      "iteration 13900: using 198.20 sec\n",
      "step 14000:train loss:4.1517\n",
      "iteration 14000: using 95.70 sec\n",
      "knn test score at 14000th iterations:  0.7502641031058526\n",
      "step 14100:train loss:4.2038\n",
      "iteration 14100: using 199.43 sec\n",
      "step 14200:train loss:4.2891\n",
      "iteration 14200: using 96.46 sec\n",
      "knn test score at 14200th iterations:  0.7484682019860553\n",
      "step 14300:train loss:3.9938\n",
      "iteration 14300: using 199.13 sec\n",
      "step 14400:train loss:4.2152\n",
      "iteration 14400: using 96.81 sec\n",
      "knn test score at 14400th iterations:  0.7477815339108388\n",
      "step 14500:train loss:4.2457\n",
      "iteration 14500: using 198.39 sec\n",
      "step 14600:train loss:4.0481\n",
      "iteration 14600: using 95.39 sec\n",
      "knn test score at 14600th iterations:  0.7488379463342489\n",
      "step 14700:train loss:3.9997\n",
      "iteration 14700: using 199.86 sec\n",
      "step 14800:train loss:4.0324\n",
      "iteration 14800: using 96.72 sec\n",
      "knn test score at 14800th iterations:  0.7477287132896683\n",
      "step 14900:train loss:4.2292\n",
      "iteration 14900: using 201.48 sec\n",
      "step 15000:train loss:3.8264\n",
      "iteration 15000: using 97.49 sec\n",
      "knn test score at 15000th iterations:  0.7486794844707374\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "knn_score = []\n",
    "train_loss_li = []\n",
    "max_score = 0.0\n",
    "for i in range(1,iterations+1):\n",
    "    encoder_inputs, decoder_inputs, seq_len_enc = mini_batch(dsamp_train, seq_len=50, input_size=75, batch_size=256)\n",
    "    _,train_loss = model.step(sess, encoder_inputs, decoder_inputs, seq_len_enc, False)\n",
    "\n",
    "    if i%100 == 0:\n",
    "        print(\"step {0}:train loss:{1:.4f}\".format(i, train_loss))\n",
    "        train_loss_li.append(train_loss)\n",
    "        end_time = timeit.default_timer()\n",
    "        print(\"iteration {}:\".format(i),end='')\n",
    "        print(\" using {:.2f} sec\".format(end_time-start_time))\n",
    "        start_time = end_time\n",
    "    # check knn score every 200 iterations\n",
    "    if i % 200 == 0:\n",
    "        knn_feature = []\n",
    "        for encoder_inputs, labels, seq_len_enc in mini_batch_classify(fea, lab, seq_len_new, batch_size=64):\n",
    "            result = get_feature(model,sess,encoder_inputs,len(encoder_inputs),seq_len_enc)\n",
    "            knn_feature.append(result)\n",
    "        test_knn_feature = []\n",
    "        for encoder_inputs, labels, seq_len_enc in mini_batch_classify(test_fea, test_lab, test_seq_len_new, batch_size=64):\n",
    "            result = get_feature(model,sess,encoder_inputs,len(encoder_inputs),seq_len_enc)\n",
    "            test_knn_feature.append(result)\n",
    "        knn_feature = np.vstack(knn_feature)\n",
    "        test_knn_feature = np.vstack(test_knn_feature)\n",
    "        neigh = KNeighborsClassifier(n_neighbors=1,metric='cosine')\n",
    "        neigh.fit(knn_feature,np.argmax(lab,axis=1))\n",
    "        score = neigh.score(test_knn_feature,np.argmax(test_lab,axis=1))\n",
    "        knn_score.append(score)\n",
    "        print(f\"knn test score at {i}th iterations: \", score)\n",
    "        # save the model: change to your own path\n",
    "        if score > max_score:\n",
    "#             model.saver.save(sess,\"/home/neuralnet/skeleton_action_recog/NTU_models/cross_subject/fixed_state/lastenc_l1\",global_step=i)\n",
    "            max_score = score\n",
    "            print(\"Current KNN Max Score is {}\".format(max_score))\n",
    "    if i%1000 == 0:\n",
    "        sess.run(model.learning_rate_decay_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
